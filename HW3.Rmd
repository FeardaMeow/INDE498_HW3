---
title: "INDE498_HW3"
author: "Steven Hwang, Haena Kim, Victoria Diaz"
date: "October 21, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)
library(dplyr)
library(boot)
library(magrittr)

ch4ex2.gpa <- read.csv("MedGPA.csv", colClasses = c("character",rep("factor",3),rep("numeric",2),rep("integer",6)))
ch4ex2.bh <- read.csv("badhealth.csv", colClasses = c("character","integer","factor","integer"))

#Remove first column
ch4ex2.gpa <- ch4ex2.gpa %>% select(-X)
ch4ex2.bh <- ch4ex2.bh %>% select(-X)
```

# Chapter 4, Exercise 1
Use AGE as the new outcome variable. Build a random forest model to predict it. Identify the final models you would select, evaluate the model, and compare it with the decision tree model. 

```{r}
library(RCurl)
library(rpart.plot)
library(rpart)
data <- read.csv(text=getURL("https://raw.githubusercontent.com/shuailab/ind_498/master/resource/data/AD.csv"))
```

We will plot the error rates associated to the number of trees in a random forest to see its impact. We see that our error rate lessens and stabilizes when the number of trees is greater than 100. We notice that one of the best looking box plots is when the number of trees is equal to 200. We will use a random forest of 200 trees in our calculations. 

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
require(randomForest)
set.seed(1)

theme_set(theme_gray(base_size = 15)) 

# define AGE_bin 
data$AGE_bin <- ifelse(data$AGE >= mean(data$AGE), 1, 0)

target_indx <- which( colnames(data) == "AGE_bin" )
data[,target_indx] <- as.factor(paste0("c", data[,target_indx]))
rm_indx <-  which( colnames(data) %in% c("ID","TOTAL13","MMSCORE", "DX_bl", "AGE") )
data <- data[,-rm_indx]

results <- NULL
for( itree in c(1:9, 10, 20, 50, 100, 200, 300, 400, 500, 600, 700)  ){
  for(i in 1:20){
      train.ix <- sample(nrow(data),floor( nrow(data)/2) )
      rf <- randomForest( AGE_bin ~ ., ntree = itree, data = data[train.ix,] ) 
      pred.test <- predict(rf, data[-train.ix,],type="class")
      this.err <-  length(which(pred.test != data[-train.ix,]$AGE_bin))/length(pred.test) 
      results <- rbind( results, c(itree, this.err)  )
  }
}

colnames(results) <- c("num_trees","error")
results <- as.data.frame(results) %>% mutate(num_trees=as.character(num_trees))
levels( results$num_trees ) <- unique( results$num_trees  )
results$num_trees <- factor( results$num_trees , unique( results$num_trees  )  )
ggplot() +
  geom_boxplot(data = results, aes(y = error, x = num_trees)) +
  geom_point(size=3) 
```


Let's create our random forest with 200 trees and analyze which variables are important. From the plot, we see that the important variables are HippoNV, FDG, AV45, and PTEDUCAT (in order from greatest to least importance). 
```{r}
# creating the random forest
random_forest_1 <- randomForest(AGE_bin ~ ., ntree = 200, data = data ) 

# plotting the importance of each variable
varImpPlot(random_forest_1)
```


We create a random forest of 200 trees with the previously defined important variables. Since the importance of the variables can change (since the predictors changed), we plot their importance yet again. We notice that all of the considered variables are important and their order of importance has stayed the same. 
We will only consider these predictors in our model from now on. 
```{r}
# create the random forest
random_forest_2 <- randomForest(AGE_bin ~ HippoNV + FDG + AV45 + PTEDUCAT, ntree = 200, data = data ) 

# plotting the importance of each variable
varImpPlot(random_forest_2)
```


We now test to see how the node size affects the error rate. We notice that there are not major changes when different node sizes are considered. We will choose to have a node size of 6, since this node size corresponds to one of the best box plots. 
```{r}
set.seed(1)

results <- NULL
for( inodesize in c(1:9, 10, 20, 50, 100)  ){
  for(i in 1:20){
      train.ix <- sample(nrow(data),floor( nrow(data)/2) )
      rf <- randomForest( AGE_bin ~ HippoNV + FDG + AV45 + PTEDUCAT, ntree = 200, nodesize = inodesize, data = data[train.ix,] ) 
      pred.test <- predict(rf, data[-train.ix,],type="class")
      this.err <-  length(which(pred.test != data[-train.ix,]$AGE_bin))/length(pred.test) 
      results <- rbind( results, c(inodesize, this.err)  )
  }
}

colnames(results) <- c("node_size","error")
results <- as.data.frame(results) %>% mutate(node_size=as.character(node_size))
levels( results$node_size ) <- unique( results$node_size  )
results$node_size <- factor( results$node_size , unique( results$node_size  )  )
ggplot() +
  geom_boxplot(data = results, aes(y = error, x = node_size)) +
  geom_point(size=3) 
```


Thus far we have considered the number of trees, the size of the nodes, and the importance of each predictor variable. Choosing the best of each of these categories, we create our final random forest. 
```{r} 
# create the random forest
random_forest_final <- randomForest(AGE_bin ~ HippoNV + FDG + AV45 + PTEDUCAT, ntree = 200, nodesize = 6, data = data ) 
```

We recall our analysis of the decision tree for predicting AGE_bin from the last homework. 
We begin by creating a decision tree. 
```{r}
data$AGE_bin <- as.factor(data$AGE_bin)
tree_1 <- rpart(AGE_bin ~., data = data)
prp(tree_1, varlen=5)
```


When we look at the variable importance of each predictor, we see that the most important variables are HippoNV and FDG. 
```{r}
print(tree_1$variable.importance)
```


Our objective is now to prune the tree. Testing different different values for cp, we see that our decision tree is most accurate when our tree has about 3 to 4 leaves. 
```{r,cache=FALSE}
library(magrittr)
library(tidyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(dplyr)
library(partykit)

set.seed(1)
train.ix <- sample(nrow(data),floor( nrow(data)/2) )
err.train.v <- NULL
err.test.v <- NULL
leaf.v <- NULL
for(i in seq(0.2,0,by=-0.005) ){
tree <- rpart( AGE_bin ~ ., data = data[train.ix,], cp=i  ) 
pred.train <- predict(tree, data[train.ix,],type="class")
pred.test <- predict(tree, data[-train.ix,],type="class")
current.err.train <- length(which(pred.train != data[train.ix,]$AGE_bin))/length(pred.train)
current.err.test <- length(which(pred.test != data[-train.ix,]$AGE_bin))/length(pred.test)
err.train.v <- c(err.train.v, current.err.train)
err.test.v <- c(err.test.v, current.err.test)
leaf.v <- c(leaf.v, length(which(tree$frame$var == "<leaf>")))
}
err.mat <- as.data.frame( cbind( train_err = err.train.v, test_err = err.test.v , leaf_num = leaf.v ) )
err.mat$leaf_num <- as.factor( err.mat$leaf_num  )
err.mat <- unique(err.mat)
err.mat <- err.mat %>% gather(type, error, train_err,test_err)

# visualizing this 
data.plot <- err.mat %>% mutate(type = factor(type))
ggplot(data.plot, aes(x=leaf_num, y=error, shape = type, color=type)) + geom_line() +
geom_point(size=3) 
```

We plot a more optimal decision tree which is only dependent on the two most important variables. 
```{r}
tree_final <- prune(tree,cp =0.0319, depth = 3)
prp(tree_final,nn.cex=1)
```

Now, we will now compare random_forest_final with tree_final. We see that the error rates with random_forest_final are less than that with tree_final. Moreover, the error rate with respect to the random forest is more centered around its mean, which means that the error rate with respect to the random forest can be estimated more accurately. 
```{r} 
err.tree <- NULL
err.rf <- NULL
for(i in 1:20){
    train.ix <- sample(nrow(data),floor( nrow(data)/2) )
    tree <- rpart( AGE_bin ~ ., data = data[train.ix,] ) 
    pred.test <- predict(tree, data[-train.ix,],type="class")
    err.tree <- c(err.tree, length(which(pred.test != data[-train.ix,]$AGE_bin))/length(pred.test) )
    
    rf <- randomForest( AGE_bin ~ ., data = data[train.ix,] ) 
    pred.test <- predict(rf, data[-train.ix,],type="class")
    err.rf <- c(err.rf, length(which(pred.test != data[-train.ix,]$AGE_bin))/length(pred.test) )
}
err.tree <- data.frame( err = err.tree , method = "tree" )
err.rf <- data.frame( err = err.rf , method = "random_forests" )

ggplot() +
  geom_boxplot(data = rbind(err.tree,err.rf), aes(y = err, x = method)) +
  geom_point(size=3)
```


# Chapter 4, Exercise 2
Find two datasets from the UCI data repository or R datasets. Conduct a detailed analysis for both datasets using the random forest model. Also comment on the application of your model on the context of the dataset you have selected. 

## Med GPA
We will plot the error rates associated to the number of trees in a random forest to see its impact. We notice that one of the best looking box plots (other than the one outlier) is when the number of trees is equal to 50. We will use a random forest of 50 trees in our calculations. 
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
require(randomForest)
set.seed(1)

# we remove the Accept column, which contains "A" (for accepted) or "D" (for denied)
ch4ex2.gpa <- ch4ex2.gpa[, -c(1)]

# we will remove the one observation which has an NA value 
ch4ex2.gpa <- na.omit(ch4ex2.gpa)

theme_set(theme_gray(base_size = 15)) 

target_indx <- which( colnames(ch4ex2.gpa) == "Acceptance" )
ch4ex2.gpa[,target_indx] <- as.factor(paste0("c", ch4ex2.gpa[,target_indx]))

results <- NULL
for( itree in c(1:9, 10, 20, 50, 100, 200, 300, 400, 500, 600, 700)  ){
  for(i in 1:20){
    train.ix <- sample(nrow(ch4ex2.gpa),floor( nrow(ch4ex2.gpa)/2) )
    rf <- randomForest( Acceptance ~ ., ntree = itree, data = ch4ex2.gpa[train.ix,] ) 
    pred.test <- predict(rf, ch4ex2.gpa[-train.ix,],type="class")
    this.err <-  length(which(pred.test != ch4ex2.gpa[-train.ix,]$Acceptance))/length(pred.test) 
    results <- rbind( results, c(itree, this.err)  )
  }
}


colnames(results) <- c("num_trees","error")
results <- as.data.frame(results) %>% mutate(num_trees=as.character(num_trees))
levels( results$num_trees ) <- unique( results$num_trees  )
results$num_trees <- factor( results$num_trees , unique( results$num_trees  )  )
ggplot() +
  geom_boxplot(data = results, aes(y = error, x = num_trees)) +
  geom_point(size=3) 
```


Let's create our random forest with 50 trees and analyze which variables are important. From the plot, we see that the most important variables are BS, GPA, and BCPM (in order from greatest to least importance). 
```{r}
# creating the random forest
random_forest_1 <- randomForest(Acceptance ~ ., ntree = 50, data = ch4ex2.gpa ) 

# plotting the importance of each variable
varImpPlot(random_forest_1)
```


We create a random forest of 50 trees with the previously defined important variables. Since the importance of the variables can change (since the predictors changed), we plot their importance yet again. We notice that all of the considered variables are even more important now, and their order of importance has changed. The order of importance of the predictors now is: grade point average (GPA), Bio/Chem/Physics/Math grade point average (BCPM), and Biological Sciences subscore (BS). 
We will only consider these predictors in our model from now on. 
```{r}
# create the random forest
random_forest_2 <- randomForest(Acceptance ~ BS + GPA + BCPM, ntree = 50, data = ch4ex2.gpa ) 

# plotting the importance of each variable
varImpPlot(random_forest_2)
```


We now test to see how the node size affects the error rate. We notice that there are not major changes when different node sizes are considered. We will choose to have a node size of 8, since this node size corresponds to one of the best box plots. 
```{r}
results <- NULL
for( inodesize in c(1:10, 15)  ){
for(i in 1:20){
train.ix <- sample(nrow(ch4ex2.gpa),floor( nrow(ch4ex2.gpa)/2) )
rf <- randomForest( Acceptance ~ BS + GPA + BCPM, ntree = 50, nodesize = inodesize, data = ch4ex2.gpa[train.ix,] ) 
pred.test <- predict(rf, ch4ex2.gpa[-train.ix,],type="class")
this.err <-  length(which(pred.test != ch4ex2.gpa[-train.ix,]$Acceptance))/length(pred.test) 
results <- rbind( results, c(inodesize, this.err)  )
}
}

colnames(results) <- c("node_size","error")
results <- as.data.frame(results) %>% mutate(node_size=as.character(node_size))
levels( results$node_size ) <- unique( results$node_size  )
results$node_size <- factor( results$node_size , unique( results$node_size  )  )
ggplot() +
geom_boxplot(data = results, aes(y = error, x = node_size)) +
geom_point(size=3) 
```


Thus far we have considered the number of trees, the size of the nodes, and the importance of each predictor variable. Choosing the best of each of these categories, we create our final random forest. 
```{r} 
# create the random forest
random_forest_final <- randomForest(Acceptance ~ BS + GPA + BCPM, ntree = 50, nodesize = 8, data = ch4ex2.gpa) 
```

We now compare the results of the random forests. The random forest with 50 trees (random_forest_1) gave us an OOB error of 29.63%; the random forest with 50 trees and 3 predictors (random_forest_2) gave us an OOB error of 22.22%; and the random forest with 50 trees, 3 predictors, and a node size of 8 (random_forest_3) gave us an OOB error of 20.37%. Moreover, random_forest_3 has the minimum number (5) of instances misclassifying class c0 and the second minimum number (6) of instances misclassifying the class c1. We conclude that random_forest_3 is the best (out of three) random forest for the given dataset. 
```{r} 
random_forest_1
random_forest_2
random_forest_3
```
This random forest sheds light on the most important predictors of a student getting accepted into a medical school. The important predictors are: grade point average, Bio/Chem/Physics/Math grade point average, and Biological Sciences subscore. Given these predictors, this model can determine whether or not a student will be accepted with an approximate 79% accuracy rate. 

## Bad Health

# Chapter 4, Exercise 3
Pick up any dataset you have used, and randomly split the data into two halves. Use one half to build the random forest model. Test the model's prediction performance on the second half. Report what you have found, adjust your way of model building, and suggest a strategy to find the model you consider as the best.

# Chapter 4, Exercise 4
Write your own R script to use Bootstrap to evaluate the significance of the regression parameters of logistic regression model. Compare your results with the output from glm().

```{r}
logCoeff.p = function(data,b,formula) {
  d <- data[b,]
  return(coefficients(glm(formula, family = binomial(link='logit'),data=d)))
}

boottest <- boot(data = ch4ex2.bh, statistic=logCoeff.p, R=100, formula=badh~numvisit+age)
#Intercept p-value
pval1 <- mean(abs(boottest$t[,1]-mean(boottest$t[,1])) > abs(boottest$t0[1]))
pval2 <- mean(abs(boottest$t[,2]-mean(boottest$t[,2])) > abs(boottest$t0[2]))
pval3 <- mean(abs(boottest$t[,3]-mean(boottest$t[,3])) > abs(boottest$t0[3]))

summary(glm(badh~., family = binomial(link='logit'),data=ch4ex2.bh))
```
# Chapter 5, Exercise 1
Find ten classification datasets from the UCI data repository or R datasets. Using these datasets, conduct experiments to see if the cross-validation method on training data can provide an approximation of the testing error on a testing data, as shown in Figure5.8. 

```{r, message=FALSE}
library(MASS)
library(datasets)
library(caret)
library(knitr)
results <- data.frame(data=character(), RMSE=double(), RMSESD=double(), testRMSE=double())
data.sets <- c("Rabbit","CO2","faithful","cats","iris","ToothGrowth","wtloss","warpbreaks","GAGurine","cars")
formula <- c("BPchange~Dose+Run+Treatment+Animal",
             "uptake~Type+Treatment+conc",
             "eruptions~waiting",
             "Bwt~Sex+Hwt",
             "Sepal.Length~Sepal.Width+Petal.Length+Petal.Width+Species",
             "len~supp+dose",
             "Weight~Days",
             "breaks~wool+tension",
             "GAG~Age",
             "dist~speed")
value <- c("BPchange","uptake","eruptions","Bwt","Sepal.Length","len","Weight","breaks","GAG","dist")
for (i in 1:10) {
  #Create initial train, test split
  trainIndex <- createDataPartition(get(data.sets[i])[,value[i]], p=0.5, list=FALSE)
  data_train <- get(data.sets[i])[trainIndex,]
  data_test <- get(data.sets[i])[-trainIndex,]
  tenfold_cv <- trainControl(method="cv", number=10)
  
  model <- train(formula(formula[i]), data=data_train, trControl=tenfold_cv, method="lm")
  test.RMSE <- predict(lm(formula(formula[i]), data=data_train), data=data_test)
  test.RMSE <- sqrt(mean((data_test[,value[i]]-test.RMSE)^2))
  temp <- data.frame(data=data.sets[i], RMSE=model$results$RMSE, RMSESD=model$results$RMSESD, testRMSE=test.RMSE)
  results <- rbind(results,temp)
}

```
```{r}
kable(results, caption = "A table of the 10-fold cross validation root mean square error and standard deviation compared to the test set root mean square error")
```
